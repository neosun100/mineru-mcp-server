#!/usr/bin/env python3
"""
MinerU æ‰¹é‡å¤„ç†å™¨ - ç”Ÿäº§çº§æ–¹æ¡ˆ
æ”¯æŒï¼šå¹¶è¡Œå¤„ç†ã€æ–‡ä»¶æ‹†åˆ†ã€ç»“æœåˆå¹¶ã€è¿›åº¦ç›‘æ§
"""
import json
import asyncio
import aiohttp
import random
from pathlib import Path
from typing import List, Dict, Optional, Callable
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import time

try:
    from PyPDF2 import PdfReader, PdfWriter
    from pptx import Presentation
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–: uv pip install PyPDF2 python-pptx python-docx")
    exit(1)


class FileChunker:
    """æ–‡ä»¶æ‹†åˆ†å™¨"""
    
    MAX_PAGES = 600  # æœ€å¤§é¡µæ•°é™åˆ¶
    
    @staticmethod
    def split_pdf(file_path: str, output_dir: str) -> List[str]:
        """æ‹†åˆ†PDF"""
        reader = PdfReader(file_path)
        total_pages = len(reader.pages)
        
        if total_pages <= FileChunker.MAX_PAGES:
            return [file_path]
        
        chunks = []
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        file_name = Path(file_path).stem
        chunk_count = (total_pages + FileChunker.MAX_PAGES - 1) // FileChunker.MAX_PAGES
        
        print(f"ğŸ“„ æ‹†åˆ†PDF: {total_pages}é¡µ â†’ {chunk_count}ä¸ªæ–‡ä»¶")
        
        for i in range(chunk_count):
            start = i * FileChunker.MAX_PAGES
            end = min((i + 1) * FileChunker.MAX_PAGES, total_pages)
            
            writer = PdfWriter()
            for page_num in range(start, end):
                writer.add_page(reader.pages[page_num])
            
            chunk_path = output_path / f"{file_name}_chunk_{i+1}.pdf"
            with open(chunk_path, 'wb') as f:
                writer.write(f)
            
            chunks.append(str(chunk_path))
            print(f"  âœ… åˆ†ç‰‡ {i+1}/{chunk_count}: {end-start}é¡µ")
        
        return chunks
    
    @staticmethod
    def split_pptx(file_path: str, output_dir: str) -> List[str]:
        """æ‹†åˆ†PPTX"""
        prs = Presentation(file_path)
        total_slides = len(prs.slides)
        
        if total_slides <= FileChunker.MAX_PAGES:
            return [file_path]
        
        chunks = []
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        file_name = Path(file_path).stem
        chunk_count = (total_slides + FileChunker.MAX_PAGES - 1) // FileChunker.MAX_PAGES
        
        print(f"ğŸ“Š æ‹†åˆ†PPTX: {total_slides}é¡µ â†’ {chunk_count}ä¸ªæ–‡ä»¶")
        
        for i in range(chunk_count):
            start = i * FileChunker.MAX_PAGES
            end = min((i + 1) * FileChunker.MAX_PAGES, total_slides)
            
            new_prs = Presentation()
            new_prs.slide_width = prs.slide_width
            new_prs.slide_height = prs.slide_height
            
            for slide_num in range(start, end):
                slide = prs.slides[slide_num]
                new_prs.slides.add_slide(slide.slide_layout)
            
            chunk_path = output_path / f"{file_name}_chunk_{i+1}.pptx"
            new_prs.save(str(chunk_path))
            
            chunks.append(str(chunk_path))
            print(f"  âœ… åˆ†ç‰‡ {i+1}/{chunk_count}: {end-start}é¡µ")
        
        return chunks
    
    @staticmethod
    def split_docx(file_path: str, output_dir: str) -> List[str]:
        """æ‹†åˆ†DOCXï¼ˆæŒ‰æ®µè½ä¼°ç®—ï¼‰"""
        doc = Document(file_path)
        total_paragraphs = len(doc.paragraphs)
        
        # ä¼°ç®—ï¼šæ¯é¡µçº¦5æ®µè½
        estimated_pages = total_paragraphs // 5
        
        if estimated_pages <= FileChunker.MAX_PAGES:
            return [file_path]
        
        chunks = []
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        file_name = Path(file_path).stem
        paras_per_chunk = FileChunker.MAX_PAGES * 5
        chunk_count = (total_paragraphs + paras_per_chunk - 1) // paras_per_chunk
        
        print(f"ğŸ“ æ‹†åˆ†DOCX: ~{estimated_pages}é¡µ â†’ {chunk_count}ä¸ªæ–‡ä»¶")
        
        for i in range(chunk_count):
            start = i * paras_per_chunk
            end = min((i + 1) * paras_per_chunk, total_paragraphs)
            
            new_doc = Document()
            for para_num in range(start, end):
                new_doc.add_paragraph(doc.paragraphs[para_num].text)
            
            chunk_path = output_path / f"{file_name}_chunk_{i+1}.docx"
            new_doc.save(str(chunk_path))
            
            chunks.append(str(chunk_path))
            print(f"  âœ… åˆ†ç‰‡ {i+1}/{chunk_count}")
        
        return chunks
    
    @staticmethod
    def split_file(file_path: str, output_dir: str = "./chunks") -> List[str]:
        """è‡ªåŠ¨è¯†åˆ«å¹¶æ‹†åˆ†æ–‡ä»¶"""
        suffix = Path(file_path).suffix.lower()
        
        if suffix == '.pdf':
            return FileChunker.split_pdf(file_path, output_dir)
        elif suffix in ['.pptx', '.ppt']:
            return FileChunker.split_pptx(file_path, output_dir)
        elif suffix in ['.docx', '.doc']:
            return FileChunker.split_docx(file_path, output_dir)
        else:
            return [file_path]


class ResultMerger:
    """ç»“æœåˆå¹¶å™¨ - å¤„ç†MinerU APIè¿”å›çš„å®Œæ•´ç»“æœ"""
    
    @staticmethod
    async def download_file(session: aiohttp.ClientSession, url: str, output_path: str):
        """ä¸‹è½½æ–‡ä»¶"""
        async with session.get(url) as resp:
            with open(output_path, 'wb') as f:
                f.write(await resp.read())
    
    @staticmethod
    async def download_and_extract_results(results: List[Dict], output_dir: str) -> List[Dict]:
        """
        ä¸‹è½½å¹¶è§£å‹æ‰€æœ‰ç»“æœ
        
        MinerU APIè¿”å›ç»“æ„:
        {
            'full_zip_url': 'https://...zip',  # å®Œæ•´å‹ç¼©åŒ…
            'md_url': 'https://...md',          # Markdownæ–‡ä»¶
            'md_content_url': 'https://...',    # Markdownå†…å®¹
            'layout_tree_url': 'https://...'    # å¸ƒå±€æ ‘
        }
        """
        import zipfile
        from pathlib import Path
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        extracted_results = []
        
        async with aiohttp.ClientSession() as session:
            for i, result in enumerate(results, 1):
                if result['status'] != 'success':
                    continue
                
                data = result['result']
                chunk_dir = output_path / f"chunk_{i}"
                chunk_dir.mkdir(exist_ok=True)
                
                print(f"ğŸ“¥ ä¸‹è½½åˆ†ç‰‡ {i}...")
                
                # ä¸‹è½½å®Œæ•´å‹ç¼©åŒ…
                if 'full_zip_url' in data:
                    zip_path = chunk_dir / "result.zip"
                    await ResultMerger.download_file(session, data['full_zip_url'], str(zip_path))
                    
                    # è§£å‹
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(chunk_dir)
                    
                    print(f"  âœ… è§£å‹å®Œæˆ: {chunk_dir}")
                
                # ä¸‹è½½Markdown
                md_content = None
                if 'md_content_url' in data:
                    async with session.get(data['md_content_url']) as resp:
                        md_content = await resp.text()
                elif 'md_url' in data:
                    async with session.get(data['md_url']) as resp:
                        md_content = await resp.text()
                
                extracted_results.append({
                    'chunk_id': i,
                    'chunk_dir': str(chunk_dir),
                    'md_content': md_content,
                    'data': data
                })
        
        return extracted_results
    
    @staticmethod
    def merge_markdown_files(extracted_results: List[Dict], output_file: str):
        """åˆå¹¶Markdownå†…å®¹"""
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, result in enumerate(extracted_results, 1):
                if i > 1:
                    f.write("\n\n" + "="*60 + "\n\n")
                
                f.write(f"# åˆ†ç‰‡ {i}\n\n")
                
                if result['md_content']:
                    f.write(result['md_content'])
                else:
                    f.write("*å†…å®¹ä¸ºç©º*\n")
        
        print(f"âœ… Markdownåˆå¹¶å®Œæˆ: {output_file}")
    
    @staticmethod
    def merge_images(extracted_results: List[Dict], output_dir: str):
        """åˆå¹¶æ‰€æœ‰å›¾ç‰‡åˆ°ç»Ÿä¸€ç›®å½•"""
        import shutil
        
        images_dir = Path(output_dir) / "images"
        images_dir.mkdir(exist_ok=True, parents=True)
        
        image_count = 0
        
        for result in extracted_results:
            chunk_dir = Path(result['chunk_dir'])
            
            # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            for img_file in chunk_dir.rglob("*.png"):
                new_name = f"chunk_{result['chunk_id']}_{img_file.name}"
                shutil.copy(img_file, images_dir / new_name)
                image_count += 1
            
            for img_file in chunk_dir.rglob("*.jpg"):
                new_name = f"chunk_{result['chunk_id']}_{img_file.name}"
                shutil.copy(img_file, images_dir / new_name)
                image_count += 1
        
        print(f"âœ… å›¾ç‰‡åˆå¹¶å®Œæˆ: {image_count} ä¸ªæ–‡ä»¶ â†’ {images_dir}")
    
    @staticmethod
    def merge_json_metadata(extracted_results: List[Dict], output_file: str):
        """åˆå¹¶JSONå…ƒæ•°æ®"""
        merged = {
            'total_chunks': len(extracted_results),
            'merged_at': datetime.now().isoformat(),
            'chunks': [
                {
                    'chunk_id': r['chunk_id'],
                    'chunk_dir': r['chunk_dir'],
                    'has_content': r['md_content'] is not None,
                    'urls': r['data']
                }
                for r in extracted_results
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(merged, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… JSONå…ƒæ•°æ®åˆå¹¶å®Œæˆ: {output_file}")


class ProgressMonitor:
    """è¿›åº¦ç›‘æ§å™¨"""
    
    def __init__(self, total: int):
        self.total = total
        self.completed = 0
        self.failed = 0
        self.start_time = time.time()
    
    def update(self, success: bool = True):
        """æ›´æ–°è¿›åº¦"""
        if success:
            self.completed += 1
        else:
            self.failed += 1
        
        self.print_progress()
    
    def print_progress(self):
        """æ‰“å°è¿›åº¦"""
        elapsed = time.time() - self.start_time
        total_done = self.completed + self.failed
        percent = (total_done / self.total) * 100
        
        print(f"\râ³ è¿›åº¦: {total_done}/{self.total} ({percent:.1f}%) | "
              f"âœ… {self.completed} | âŒ {self.failed} | "
              f"â±ï¸  {elapsed:.1f}s", end='', flush=True)
        
        if total_done == self.total:
            print()  # æ¢è¡Œ


class MinerUBatchProcessor:
    """MinerU æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, tokens_file='all_tokens.json', max_workers=10):
        """
        åˆå§‹åŒ–
        
        Args:
            tokens_file: Tokenæ–‡ä»¶
            max_workers: æœ€å¤§å¹¶è¡Œåº¦
        """
        self.tokens_file = tokens_file
        self.max_workers = max_workers
        self.tokens = self._load_tokens()
        self.base_url = 'https://mineru.net/api/v4'
        
        if not self.tokens:
            raise ValueError("æœªæ‰¾åˆ°Tokenï¼Œè¯·å…ˆè¿è¡Œ batch_login.py")
        
        print(f"âœ… å·²åŠ è½½ {len(self.tokens)} ä¸ªè´¦æˆ·")
        print(f"âš™ï¸  æœ€å¤§å¹¶è¡Œåº¦: {max_workers}")
    
    def _load_tokens(self) -> Dict:
        """åŠ è½½Token"""
        try:
            with open(self.tokens_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _get_random_token(self) -> str:
        """éšæœºé€‰æ‹©Token"""
        email = random.choice(list(self.tokens.keys()))
        return self.tokens[email]['token']
    
    async def _process_single_file(self, session: aiohttp.ClientSession, 
                                   file_url: str, file_id: str) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        # åˆ›å»ºä»»åŠ¡
        data = {'url': file_url, 'model_version': 'vlm'}
        
        async with session.post(
            f"{self.base_url}/extract/task",
            headers=headers,
            json=data
        ) as resp:
            result = await resp.json()
            
            if result['code'] != 0:
                return {'file_id': file_id, 'status': 'failed', 'error': result}
            
            task_id = result['data']['task_id']
        
        # è½®è¯¢ç»“æœ
        max_attempts = 60
        for _ in range(max_attempts):
            await asyncio.sleep(5)
            
            async with session.get(
                f"{self.base_url}/extract/task/{task_id}",
                headers=headers
            ) as resp:
                result = await resp.json()
                
                if result['code'] == 0:
                    data = result['data']
                    state = data.get('state')
                    
                    if state == 'done':
                        return {
                            'file_id': file_id,
                            'status': 'success',
                            'task_id': task_id,
                            'result': data
                        }
                    elif state == 'failed':
                        return {
                            'file_id': file_id,
                            'status': 'failed',
                            'error': data.get('err_msg')
                        }
        
        return {'file_id': file_id, 'status': 'timeout'}
    
    async def process_files_async(self, files: List[Dict]) -> List[Dict]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ–‡ä»¶"""
        monitor = ProgressMonitor(len(files))
        
        async with aiohttp.ClientSession() as session:
            semaphore = asyncio.Semaphore(self.max_workers)
            
            async def process_with_semaphore(file_info):
                async with semaphore:
                    result = await self._process_single_file(
                        session,
                        file_info['url'],
                        file_info['id']
                    )
                    monitor.update(result['status'] == 'success')
                    return result
            
            tasks = [process_with_semaphore(f) for f in files]
            results = await asyncio.gather(*tasks)
        
        return results
    
    def process_files(self, files: List[Dict]) -> List[Dict]:
        """æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆåŒæ­¥å…¥å£ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
        print(f"âš™ï¸  å¹¶è¡Œåº¦: {self.max_workers}")
        print()
        
        results = asyncio.run(self.process_files_async(files))
        
        # ç»Ÿè®¡
        success = sum(1 for r in results if r['status'] == 'success')
        failed = sum(1 for r in results if r['status'] == 'failed')
        timeout = sum(1 for r in results if r['status'] == 'timeout')
        
        print(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
        print(f"  âœ… æˆåŠŸ: {success}")
        print(f"  âŒ å¤±è´¥: {failed}")
        print(f"  â±ï¸  è¶…æ—¶: {timeout}")
        
        return results
    
    def process_large_file(self, file_path: str, output_dir: str = "./output") -> Dict:
        """
        å¤„ç†å¤§æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ‹†åˆ†ã€å¹¶è¡Œå¤„ç†ã€å®Œæ•´åˆå¹¶ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"\nğŸ“„ å¤„ç†å¤§æ–‡ä»¶: {file_path}")
        
        # 1. æ‹†åˆ†æ–‡ä»¶
        chunks = FileChunker.split_file(file_path, "./chunks")
        print(f"ğŸ“¦ æ‹†åˆ†å®Œæˆ: {len(chunks)} ä¸ªåˆ†ç‰‡")
        
        if len(chunks) == 1:
            print("ğŸ’¡ æ–‡ä»¶æ— éœ€æ‹†åˆ†ï¼Œç›´æ¥å¤„ç†")
        
        # 2. ä¸Šä¼ åˆ†ç‰‡ï¼ˆè¿™é‡Œéœ€è¦å®é™…ä¸Šä¼ é€»è¾‘ï¼‰
        # TODO: å®ç°æ–‡ä»¶ä¸Šä¼ åˆ°CDN
        print("\nâš ï¸  æ³¨æ„: éœ€è¦å…ˆä¸Šä¼ åˆ†ç‰‡åˆ°CDNï¼Œè·å–URL")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ upload_chunks() æ–¹æ³•ä¸Šä¼ ")
        
        files = [
            {'id': f'chunk_{i}', 'url': f'https://example.com/{Path(c).name}'}
            for i, c in enumerate(chunks)
        ]
        
        # 3. å¹¶è¡Œå¤„ç†
        results = self.process_files(files)
        
        # 4. ä¸‹è½½å¹¶è§£å‹æ‰€æœ‰ç»“æœ
        print(f"\nğŸ“¥ ä¸‹è½½å¹¶è§£å‹ç»“æœ...")
        extracted_results = asyncio.run(
            ResultMerger.download_and_extract_results(results, output_dir)
        )
        
        # 5. åˆå¹¶æ‰€æœ‰å†…å®¹
        output_path = Path(output_dir)
        file_name = Path(file_path).stem
        
        print(f"\nğŸ”— åˆå¹¶ç»“æœ...")
        
        # åˆå¹¶Markdown
        md_file = output_path / f"{file_name}_merged.md"
        ResultMerger.merge_markdown_files(extracted_results, str(md_file))
        
        # åˆå¹¶å›¾ç‰‡
        ResultMerger.merge_images(extracted_results, output_dir)
        
        # åˆå¹¶å…ƒæ•°æ®
        json_file = output_path / f"{file_name}_metadata.json"
        ResultMerger.merge_json_metadata(extracted_results, str(json_file))
        
        return {
            'total_chunks': len(chunks),
            'success': len(extracted_results),
            'failed': len(results) - len(extracted_results),
            'output_files': {
                'markdown': str(md_file),
                'images': str(output_path / "images"),
                'metadata': str(json_file)
            }
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print("  æ‰¹é‡å¤„ç†: python3 mineru_batch_processor.py batch <file1> <file2> ...")
        print("  å¤§æ–‡ä»¶å¤„ç†: python3 mineru_batch_processor.py large <file>")
        sys.exit(1)
    
    mode = sys.argv[1]
    processor = MinerUBatchProcessor(max_workers=10)
    
    if mode == 'batch':
        # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        files = [
            {'id': f'file_{i}', 'url': f'https://example.com/{Path(f).name}'}
            for i, f in enumerate(sys.argv[2:])
        ]
        processor.process_files(files)
    
    elif mode == 'large':
        # å¤„ç†å¤§æ–‡ä»¶
        file_path = sys.argv[2]
        result = processor.process_large_file(file_path)
        print(f"\nâœ… å¤„ç†å®Œæˆ: {result}")
