#!/usr/bin/env python3
"""
MinerU ç”Ÿäº§çº§å®¢æˆ·ç«¯ - å®Œæ•´è§£å†³æ–¹æ¡ˆ
æ”¯æŒï¼šæ™ºèƒ½æ‹†åˆ†ã€å¹¶è¡Œå¤„ç†ã€å®Œæ•´åˆå¹¶ã€è¿›åº¦ç›‘æ§
ç›´æ¥ä½¿ç”¨APIï¼Œä¸ä¾èµ–å®˜æ–¹SDK
"""
import json
import asyncio
import aiohttp
import random
import time
import zipfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

try:
    from PyPDF2 import PdfReader, PdfWriter
    from pptx import Presentation
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–:")
    print("   uv pip install PyPDF2 python-pptx python-docx aiohttp")
    exit(1)


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨ - æ”¯æŒæœ¬åœ°æ–‡ä»¶å’ŒURL"""
    
    MAX_SIZE = 200 * 1024 * 1024  # 200MB
    MAX_PAGES = 600
    
    SUPPORTED_FORMATS = {
        'pdf': 'application/pdf',
        'doc': 'application/msword',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'ppt': 'application/vnd.ms-powerpoint',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'html': 'text/html'
    }
    
    @staticmethod
    def is_url(path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºURL"""
        return path.startswith(('http://', 'https://'))
    
    @staticmethod
    async def validate_url(session: aiohttp.ClientSession, url: str) -> Tuple[bool, str, Dict]:
        """
        éªŒè¯URL
        
        Returns:
            (is_valid, error_msg, file_info)
        """
        try:
            # HEADè¯·æ±‚è·å–æ–‡ä»¶ä¿¡æ¯
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status != 200:
                    return False, f"URLæ— æ³•è®¿é—®: {resp.status}", {}
                
                # è·å–æ–‡ä»¶å¤§å°
                size = int(resp.headers.get('content-length', 0))
                if size > FileValidator.MAX_SIZE:
                    return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
                
                # ä»URLæˆ–Content-Typeæ¨æ–­æ ¼å¼
                content_type = resp.headers.get('content-type', '')
                format = FileValidator._guess_format_from_url(url, content_type)
                
                if not format:
                    return False, f"æ— æ³•è¯†åˆ«æ–‡ä»¶æ ¼å¼", {}
                
                file_info = {
                    'path': url,
                    'name': Path(url).name or 'document',
                    'size': size,
                    'format': format,
                    'is_url': True,
                    'pages': None,  # URLæ— æ³•é¢„å…ˆè·å–é¡µæ•°
                    'needs_split': False  # ä½¿ç”¨page_rangeså‚æ•°
                }
                
                return True, "", file_info
        
        except Exception as e:
            return False, f"URLéªŒè¯å¤±è´¥: {e}", {}
    
    @staticmethod
    def _guess_format_from_url(url: str, content_type: str) -> Optional[str]:
        """ä»URLå’ŒContent-Typeæ¨æ–­æ ¼å¼"""
        # ä»URLæ‰©å±•åæ¨æ–­
        url_lower = url.lower()
        for ext in FileValidator.SUPPORTED_FORMATS.keys():
            if url_lower.endswith(f'.{ext}'):
                return ext
        
        # ä»Content-Typeæ¨æ–­
        for ext, mime in FileValidator.SUPPORTED_FORMATS.items():
            if mime in content_type:
                return ext
        
        return None
    
    @staticmethod
    def validate_file(file_path: str) -> Tuple[bool, str, Dict]:
        """
        éªŒè¯æœ¬åœ°æ–‡ä»¶
        
        Returns:
            (is_valid, error_msg, file_info)
        """
        path = Path(file_path)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨", {}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = path.stat().st_size
        if size > FileValidator.MAX_SIZE:
            return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
        
        if size == 0:
            return False, "æ–‡ä»¶ä¸ºç©º", {}
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        suffix = path.suffix.lower().lstrip('.')
        if suffix not in FileValidator.SUPPORTED_FORMATS:
            return False, f"ä¸æ”¯æŒçš„æ ¼å¼: {suffix}", {}
        
        # æ£€æŸ¥é¡µæ•°
        pages = FileValidator._get_page_count(file_path, suffix)
        
        file_info = {
            'path': str(path),
            'name': path.name,
            'size': size,
            'format': suffix,
            'is_url': False,
            'pages': pages,
            'needs_split': pages > FileValidator.MAX_PAGES if pages else False
        }
        
        return True, "", file_info
    
    @staticmethod
    def _get_page_count(file_path: str, format: str) -> Optional[int]:
        """è·å–é¡µæ•°"""
        try:
            if format == 'pdf':
                reader = PdfReader(file_path)
                return len(reader.pages)
            elif format in ['pptx', 'ppt']:
                prs = Presentation(file_path)
                return len(prs.slides)
            elif format in ['docx', 'doc']:
                # DOCXé¡µæ•°ä¼°ç®—
                doc = Document(file_path)
                return len(doc.paragraphs) // 5  # ä¼°ç®—ï¼š5æ®µ/é¡µ
        except:
            pass
        return None


class SmartChunker:
    """æ™ºèƒ½æ‹†åˆ†å™¨ - æ”¯æŒpage_rangeså‚æ•°"""
    
    @staticmethod
    def create_chunks_with_ranges(file_info: Dict) -> List[Dict]:
        """
        åˆ›å»ºåˆ†ç‰‡é…ç½®ï¼ˆä½¿ç”¨page_rangeså‚æ•°ï¼‰
        
        Returns:
            [{'file_path': '...', 'page_ranges': '1-600'}, ...]
        """
        pages = file_info['pages']
        if not pages or pages <= FileValidator.MAX_PAGES:
            return [{'file_path': file_info['path'], 'page_ranges': None}]
        
        chunks = []
        chunk_size = FileValidator.MAX_PAGES
        
        for i in range(0, pages, chunk_size):
            start = i + 1  # é¡µç ä»1å¼€å§‹
            end = min(i + chunk_size, pages)
            
            chunks.append({
                'file_path': file_info['path'],
                'page_ranges': f"{start}-{end}",
                'chunk_id': len(chunks) + 1,
                'pages': end - start + 1
            })
        
        print(f"ğŸ“„ æ–‡ä»¶æ‹†åˆ†: {pages}é¡µ â†’ {len(chunks)}ä¸ªåˆ†ç‰‡")
        for chunk in chunks:
            print(f"  åˆ†ç‰‡{chunk['chunk_id']}: é¡µç  {chunk['page_ranges']} ({chunk['pages']}é¡µ)")
        
        return chunks


class MinerUClient:
    """MinerU API å®¢æˆ·ç«¯"""
    
    def __init__(self, tokens_file='all_tokens.json'):
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not Path(tokens_file).is_absolute():
            # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•
            script_dir = Path(__file__).parent
            tokens_file = script_dir / tokens_file
        
        self.tokens_file = str(tokens_file)
        self.tokens = self._load_tokens()
        self.base_url = 'https://mineru.net/api/v4'
        
        if not self.tokens:
            raise ValueError(f"æœªæ‰¾åˆ°Tokenæ–‡ä»¶: {self.tokens_file}")
        
        print(f"âœ… å·²åŠ è½½ {len(self.tokens)} ä¸ªè´¦æˆ·")
    
    def _load_tokens(self) -> Dict:
        """åŠ è½½Token"""
        try:
            with open(self.tokens_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _get_random_token(self) -> str:
        """éšæœºé€‰æ‹©Token"""
        email = random.choice(list(self.tokens.keys()))
        return self.tokens[email]['token']
    
    async def create_task(self, session: aiohttp.ClientSession, 
                         file_url: str, **options) -> Optional[str]:
        """
        åˆ›å»ºè§£æä»»åŠ¡
        
        Args:
            file_url: æ–‡ä»¶URL
            **options: å…¶ä»–å‚æ•°ï¼ˆpage_ranges, model_versionç­‰ï¼‰
        
        Returns:
            task_id
        """
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        data = {'url': file_url, **options}
        
        async with session.post(
            f"{self.base_url}/extract/task",
            headers=headers,
            json=data,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']['task_id']
            else:
                print(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {result.get('msg')}")
                return None
    
    async def get_task_result(self, session: aiohttp.ClientSession, 
                             task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}'
        }
        
        async with session.get(
            f"{self.base_url}/extract/task/{task_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']
            return None
    
    async def upload_local_file(self, session: aiohttp.ClientSession,
                               file_path: str, **options) -> Optional[str]:
        """
        ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°CDN
        
        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            **options: APIå‚æ•°
        
        Returns:
            batch_id
        """
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        file_name = Path(file_path).name
        
        # 1. è·å–ä¸Šä¼ é“¾æ¥
        data = {
            'files': [{'name': file_name}],
            **options
        }
        
        async with session.post(
            f"{self.base_url}/file-urls/batch",
            headers=headers,
            json=data,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] != 0:
                print(f"âŒ è·å–ä¸Šä¼ é“¾æ¥å¤±è´¥: {result.get('msg')}")
                return None
            
            batch_id = result['data']['batch_id']
            upload_url = result['data']['file_urls'][0]
            print(f"âœ… è·å–ä¸Šä¼ é“¾æ¥æˆåŠŸ")
        
        # 2. ä¸Šä¼ æ–‡ä»¶
        print(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶ä¸­...")
        with open(file_path, 'rb') as f:
            file_data = f.read()
        
        async with session.put(
            upload_url,
            data=file_data,
            timeout=aiohttp.ClientTimeout(total=300)
        ) as resp:
            if resp.status == 200:
                print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
                return batch_id
            else:
                print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {resp.status}")
                return None
    
    async def get_batch_result(self, session: aiohttp.ClientSession,
                               batch_id: str) -> Optional[List[Dict]]:
        """è·å–æ‰¹é‡ä»»åŠ¡ç»“æœ"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}'
        }
        
        async with session.get(
            f"{self.base_url}/extract-results/batch/{batch_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']['extract_result']
            return None
    
    async def wait_for_batch_completion(self, session: aiohttp.ClientSession,
                                       batch_id: str, max_wait: int = 600) -> Optional[List[Dict]]:
        """ç­‰å¾…æ‰¹é‡ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            results = await self.get_batch_result(session, batch_id)
            
            if results:
                all_done = True
                for result in results:
                    state = result.get('state')
                    
                    if state == 'failed':
                        print(f"âŒ æ–‡ä»¶å¤±è´¥: {result.get('file_name')} - {result.get('err_msg')}")
                        return None
                    elif state in ['pending', 'running', 'waiting-file', 'converting']:
                        all_done = False
                        if state == 'running':
                            progress = result.get('extract_progress', {})
                            extracted = progress.get('extracted_pages', 0)
                            total = progress.get('total_pages', 0)
                            if total > 0:
                                print(f"  è¿›åº¦: {extracted}/{total}é¡µ", end='\r')
                
                if all_done:
                    return results
            
            await asyncio.sleep(5)
        
        print(f"âŒ ä»»åŠ¡è¶…æ—¶")
        return None


class ResultProcessor:
    """ç»“æœå¤„ç†å™¨ - ä¸‹è½½ã€è§£å‹ã€åˆå¹¶"""
    
    @staticmethod
    async def download_and_extract(session: aiohttp.ClientSession,
                                   zip_url: str, output_dir: str) -> Optional[str]:
        """ä¸‹è½½å¹¶è§£å‹ç»“æœ"""
        try:
            # ä¸‹è½½
            async with session.get(zip_url, timeout=aiohttp.ClientTimeout(total=300)) as resp:
                zip_data = await resp.read()
            
            # ä¿å­˜
            zip_path = Path(output_dir) / "result.zip"
            with open(zip_path, 'wb') as f:
                f.write(zip_data)
            
            # è§£å‹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
            
            # åˆ é™¤zip
            zip_path.unlink()
            
            return output_dir
        except Exception as e:
            print(f"âŒ ä¸‹è½½è§£å‹å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def find_markdown(chunk_dir: str) -> Optional[str]:
        """æŸ¥æ‰¾Markdownæ–‡ä»¶"""
        for md_file in Path(chunk_dir).rglob("*.md"):
            return str(md_file)
        return None
    
    @staticmethod
    def merge_results(chunk_dirs: List[str], output_file: str):
        """åˆå¹¶æ‰€æœ‰Markdown"""
        with open(output_file, 'w', encoding='utf-8') as out:
            for i, chunk_dir in enumerate(chunk_dirs, 1):
                if i > 1:
                    out.write("\n\n" + "="*60 + "\n\n")
                
                out.write(f"# åˆ†ç‰‡ {i}\n\n")
                
                md_file = ResultProcessor.find_markdown(chunk_dir)
                if md_file:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        out.write(f.read())
                else:
                    out.write("*å†…å®¹ä¸ºç©º*\n")
        
        print(f"âœ… Markdownåˆå¹¶å®Œæˆ: {output_file}")
    
    @staticmethod
    def merge_images(chunk_dirs: List[str], output_dir: str):
        """åˆå¹¶æ‰€æœ‰å›¾ç‰‡"""
        images_dir = Path(output_dir) / "images"
        images_dir.mkdir(exist_ok=True, parents=True)
        
        count = 0
        for i, chunk_dir in enumerate(chunk_dirs, 1):
            for img in Path(chunk_dir).rglob("*.png"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
            
            for img in Path(chunk_dir).rglob("*.jpg"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
        
        print(f"âœ… å›¾ç‰‡åˆå¹¶å®Œæˆ: {count}ä¸ªæ–‡ä»¶ â†’ {images_dir}")


class MinerUProcessor:
    """MinerU å®Œæ•´å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.client = MinerUClient()
        self.max_workers = max_workers
    
    async def process_file(self, file_path: str, output_dir: str = "./output",
                          **options) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå®Œæ•´æµç¨‹ï¼‰- æ”¯æŒæœ¬åœ°æ–‡ä»¶å’ŒURL
        """
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"process_file() å¼€å§‹: {file_path}")
        print(f"\nğŸ“„ å¤„ç†: {file_path}")
        
        # 1. éªŒè¯æ–‡ä»¶æˆ–URL
        try:
            if FileValidator.is_url(file_path):
                logger.info("æ£€æµ‹åˆ°URL")
                print("ğŸŒ æ£€æµ‹åˆ°URLï¼ŒéªŒè¯ä¸­...")
                async with aiohttp.ClientSession() as session:
                    is_valid, error, file_info = await FileValidator.validate_url(session, file_path)
                file_url = file_path
                batch_id = None
            else:
                logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶")
                print("ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ï¼ŒéªŒè¯ä¸­...")
                is_valid, error, file_info = FileValidator.validate_file(file_path)
                file_url = None
                batch_id = None
            
            logger.info(f"éªŒè¯ç»“æœ: is_valid={is_valid}, error={error}")
            
            if not is_valid:
                logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥: {error}")
                print(f"âŒ {error}")
                return None
            
            logger.info(f"æ–‡ä»¶ä¿¡æ¯: {file_info}")
            print(f"âœ… éªŒè¯é€šè¿‡: {file_info['format'].upper()}, {file_info['size']/1024/1024:.1f}MB")
            if file_info.get('pages'):
                print(f"   é¡µæ•°: {file_info['pages']}")
        
        except Exception as e:
            logger.error(f"éªŒè¯é˜¶æ®µå¼‚å¸¸: {e}", exc_info=True)
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            return None
        
        # 2. å¤„ç†æœ¬åœ°æ–‡ä»¶ï¼šä¸Šä¼ åˆ°CDN
        logger.info("æ­¥éª¤2: å¼€å§‹å¤„ç†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ ...")
        try:
            async with aiohttp.ClientSession() as session:
                if not file_info['is_url']:
                    logger.info("æœ¬åœ°æ–‡ä»¶ï¼Œéœ€è¦ä¸Šä¼ ")
                    print(f"\nğŸ“¤ ä¸Šä¼ æœ¬åœ°æ–‡ä»¶...")
                    
                    # è®¾ç½®é»˜è®¤å‚æ•°
                    upload_options = {
                        'model_version': options.get('model_version', 'vlm'),
                        'enable_formula': options.get('enable_formula', True),
                        'enable_table': options.get('enable_table', True)
                    }
                    logger.info(f"ä¸Šä¼ é€‰é¡¹: {upload_options}")
                    
                    batch_id = await self.client.upload_local_file(
                        session,
                        file_path,
                        **upload_options
                    )
                    
                    logger.info(f"ä¸Šä¼ ç»“æœ: batch_id={batch_id}")
                    
                    if not batch_id:
                        logger.error("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                        print("âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                        return None
                    
                    print(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ ï¼Œbatch_id: {batch_id}")
                else:
                    logger.info("URLæ–‡ä»¶ï¼Œæ— éœ€ä¸Šä¼ ")
            
            # 3. ç­‰å¾…å¤„ç†å®Œæˆ
            if batch_id:
                # æœ¬åœ°æ–‡ä»¶ï¼šä½¿ç”¨batch_idæŸ¥è¯¢
                print(f"\nâ³ ç­‰å¾…å¤„ç†å®Œæˆ...")
                results = await self.client.wait_for_batch_completion(session, batch_id)
                
                if not results or len(results) == 0:
                    print("âŒ å¤„ç†å¤±è´¥")
                    return None
                
                result = results[0]  # å•æ–‡ä»¶åªæœ‰ä¸€ä¸ªç»“æœ
                
                if result.get('state') != 'done':
                    print(f"âŒ å¤„ç†å¤±è´¥: {result.get('err_msg')}")
                    return None
                
                full_zip_url = result.get('full_zip_url')
            else:
                # URLï¼šä½¿ç”¨page_rangeså¤„ç†
                chunks = SmartChunker.create_chunks_with_ranges(file_info)
                
                print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†ç‰‡...")
                
                tasks = []
                for chunk in chunks:
                    task_options = {**options}
                    if chunk['page_ranges']:
                        task_options['page_ranges'] = chunk['page_ranges']
                    
                    task = self._process_chunk(session, file_url, chunk, task_options)
                    tasks.append(task)
                
                chunk_results = await asyncio.gather(*tasks)
                
                # è¿™é‡Œç®€åŒ–ï¼šåªå¤„ç†ç¬¬ä¸€ä¸ªåˆ†ç‰‡çš„ç»“æœ
                success_results = [r for r in chunk_results if r]
                if not success_results:
                    print("âŒ æ‰€æœ‰åˆ†ç‰‡å¤„ç†å¤±è´¥")
                    return None
                
                full_zip_url = success_results[0]['full_zip_url']
            
            # 4. ä¸‹è½½å¹¶è§£å‹ç»“æœ
            print(f"\nğŸ“¥ ä¸‹è½½å¹¶è§£å‹ç»“æœ...")
            output_path = Path(output_dir)
            if not file_info['is_url']:
                # æœ¬åœ°æ–‡ä»¶ï¼šè¾“å‡ºåˆ°åŒç›®å½•
                output_path = Path(file_path).parent
            
            output_path.mkdir(exist_ok=True, parents=True)
            
            chunk_dir = output_path / f"{Path(file_path).stem}_result"
            chunk_dir.mkdir(exist_ok=True)
            
            extracted = await ResultProcessor.download_and_extract(
                session,
                full_zip_url,
                str(chunk_dir)
            )
            
            if not extracted:
                print("âŒ ä¸‹è½½è§£å‹å¤±è´¥")
                return None
            
            # 5. æ•´ç†è¾“å‡º
            file_name = Path(file_path).stem
            md_file = output_path / f"{file_name}.md"
            images_dir = output_path / f"{file_name}_images"
            
            # å¤åˆ¶Markdown
            source_md = ResultProcessor.find_markdown(extracted)
            if source_md:
                shutil.copy(source_md, md_file)
                print(f"âœ… Markdown: {md_file}")
            
            # å¤åˆ¶å›¾ç‰‡
            source_images = Path(extracted) / "images"
            if source_images.exists():
                if images_dir.exists():
                    shutil.rmtree(images_dir)
                shutil.copytree(source_images, images_dir)
                image_count = len(list(images_dir.glob("*")))
                print(f"âœ… å›¾ç‰‡: {images_dir} ({image_count}ä¸ª)")
            
            return {
                'source': file_path,
                'source_type': 'url' if file_info['is_url'] else 'file',
                'output': {
                    'markdown': str(md_file),
                    'images': str(images_dir) if images_dir.exists() else None
                }
            }
        
        except Exception as e:
            logger.error(f"å¤„ç†é˜¶æ®µå¼‚å¸¸: {e}", exc_info=True)
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return None
    
    async def _process_chunk(self, session: aiohttp.ClientSession,
                            file_url: str, chunk: Dict, options: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªåˆ†ç‰‡"""
        chunk_id = chunk.get('chunk_id', 1)
        print(f"  åˆ†ç‰‡{chunk_id}: åˆ›å»ºä»»åŠ¡...")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = await self.client.create_task(session, file_url, **options)
        if not task_id:
            return None
        
        print(f"  åˆ†ç‰‡{chunk_id}: ç­‰å¾…å®Œæˆ...")
        
        # ç­‰å¾…å®Œæˆ
        result = await self.client.wait_for_completion(session, task_id)
        
        if result:
            print(f"  âœ… åˆ†ç‰‡{chunk_id}: å®Œæˆ")
        else:
            print(f"  âŒ åˆ†ç‰‡{chunk_id}: å¤±è´¥")
        
        return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 mineru_production.py <file_path> [options]")
        print("\né€‰é¡¹:")
        print("  --model-version vlm|pipeline|MinerU-HTML")
        print("  --is-ocr true|false")
        print("  --enable-formula true|false")
        print("  --enable-table true|false")
        print("  --language ch|en|...")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    # è§£æé€‰é¡¹
    options = {}
    for i in range(2, len(sys.argv), 2):
        if i + 1 < len(sys.argv):
            key = sys.argv[i].lstrip('--').replace('-', '_')
            value = sys.argv[i + 1]
            
            # è½¬æ¢å¸ƒå°”å€¼
            if value.lower() in ['true', 'false']:
                value = value.lower() == 'true'
            
            options[key] = value
    
    # å¤„ç†æ–‡ä»¶
    processor = MinerUProcessor(max_workers=10)
    result = asyncio.run(processor.process_file(file_path, **options))
    
    if result:
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"  æ€»åˆ†ç‰‡: {result['total_chunks']}")
        print(f"  æˆåŠŸ: {result['success']}")
        print(f"  å¤±è´¥: {result['failed']}")
        print(f"  è¾“å‡º: {result['output']}")
