#!/usr/bin/env python3
"""
MinerU ç”Ÿäº§çº§å®¢æˆ·ç«¯ - å®Œæ•´è§£å†³æ–¹æ¡ˆ
æ”¯æŒï¼šæ™ºèƒ½æ‹†åˆ†ã€å¹¶è¡Œå¤„ç†ã€å®Œæ•´åˆå¹¶ã€è¿›åº¦ç›‘æ§
ç›´æ¥ä½¿ç”¨APIï¼Œä¸ä¾èµ–å®˜æ–¹SDK
"""
import json
import asyncio
import aiohttp
import random
import time
import zipfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

try:
    from PyPDF2 import PdfReader, PdfWriter
    from pptx import Presentation
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–:")
    print("   uv pip install PyPDF2 python-pptx python-docx aiohttp")
    exit(1)


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨"""
    
    MAX_SIZE = 200 * 1024 * 1024  # 200MB
    MAX_PAGES = 600
    
    SUPPORTED_FORMATS = {
        'pdf': 'application/pdf',
        'doc': 'application/msword',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'ppt': 'application/vnd.ms-powerpoint',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'html': 'text/html'
    }
    
    @staticmethod
    def validate_file(file_path: str) -> Tuple[bool, str, Dict]:
        """
        éªŒè¯æ–‡ä»¶
        
        Returns:
            (is_valid, error_msg, file_info)
        """
        path = Path(file_path)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨", {}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = path.stat().st_size
        if size > FileValidator.MAX_SIZE:
            return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
        
        if size == 0:
            return False, "æ–‡ä»¶ä¸ºç©º", {}
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        suffix = path.suffix.lower().lstrip('.')
        if suffix not in FileValidator.SUPPORTED_FORMATS:
            return False, f"ä¸æ”¯æŒçš„æ ¼å¼: {suffix}", {}
        
        # æ£€æŸ¥é¡µæ•°
        pages = FileValidator._get_page_count(file_path, suffix)
        
        file_info = {
            'path': str(path),
            'name': path.name,
            'size': size,
            'format': suffix,
            'pages': pages,
            'needs_split': pages > FileValidator.MAX_PAGES if pages else False
        }
        
        return True, "", file_info
    
    @staticmethod
    def _get_page_count(file_path: str, format: str) -> Optional[int]:
        """è·å–é¡µæ•°"""
        try:
            if format == 'pdf':
                reader = PdfReader(file_path)
                return len(reader.pages)
            elif format in ['pptx', 'ppt']:
                prs = Presentation(file_path)
                return len(prs.slides)
            elif format in ['docx', 'doc']:
                # DOCXé¡µæ•°ä¼°ç®—
                doc = Document(file_path)
                return len(doc.paragraphs) // 5  # ä¼°ç®—ï¼š5æ®µ/é¡µ
        except:
            pass
        return None


class SmartChunker:
    """æ™ºèƒ½æ‹†åˆ†å™¨ - æ”¯æŒpage_rangeså‚æ•°"""
    
    @staticmethod
    def create_chunks_with_ranges(file_info: Dict) -> List[Dict]:
        """
        åˆ›å»ºåˆ†ç‰‡é…ç½®ï¼ˆä½¿ç”¨page_rangeså‚æ•°ï¼‰
        
        Returns:
            [{'file_path': '...', 'page_ranges': '1-600'}, ...]
        """
        pages = file_info['pages']
        if not pages or pages <= FileValidator.MAX_PAGES:
            return [{'file_path': file_info['path'], 'page_ranges': None}]
        
        chunks = []
        chunk_size = FileValidator.MAX_PAGES
        
        for i in range(0, pages, chunk_size):
            start = i + 1  # é¡µç ä»1å¼€å§‹
            end = min(i + chunk_size, pages)
            
            chunks.append({
                'file_path': file_info['path'],
                'page_ranges': f"{start}-{end}",
                'chunk_id': len(chunks) + 1,
                'pages': end - start + 1
            })
        
        print(f"ğŸ“„ æ–‡ä»¶æ‹†åˆ†: {pages}é¡µ â†’ {len(chunks)}ä¸ªåˆ†ç‰‡")
        for chunk in chunks:
            print(f"  åˆ†ç‰‡{chunk['chunk_id']}: é¡µç  {chunk['page_ranges']} ({chunk['pages']}é¡µ)")
        
        return chunks


class MinerUClient:
    """MinerU API å®¢æˆ·ç«¯"""
    
    def __init__(self, tokens_file='all_tokens.json'):
        self.tokens_file = tokens_file
        self.tokens = self._load_tokens()
        self.base_url = 'https://mineru.net/api/v4'
        
        if not self.tokens:
            raise ValueError("æœªæ‰¾åˆ°Tokenï¼Œè¯·å…ˆè¿è¡Œ batch_login.py")
        
        print(f"âœ… å·²åŠ è½½ {len(self.tokens)} ä¸ªè´¦æˆ·")
    
    def _load_tokens(self) -> Dict:
        """åŠ è½½Token"""
        try:
            with open(self.tokens_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _get_random_token(self) -> str:
        """éšæœºé€‰æ‹©Token"""
        email = random.choice(list(self.tokens.keys()))
        return self.tokens[email]['token']
    
    async def create_task(self, session: aiohttp.ClientSession, 
                         file_url: str, **options) -> Optional[str]:
        """
        åˆ›å»ºè§£æä»»åŠ¡
        
        Args:
            file_url: æ–‡ä»¶URL
            **options: å…¶ä»–å‚æ•°ï¼ˆpage_ranges, model_versionç­‰ï¼‰
        
        Returns:
            task_id
        """
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        data = {'url': file_url, **options}
        
        async with session.post(
            f"{self.base_url}/extract/task",
            headers=headers,
            json=data,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']['task_id']
            else:
                print(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {result.get('msg')}")
                return None
    
    async def get_task_result(self, session: aiohttp.ClientSession, 
                             task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}'
        }
        
        async with session.get(
            f"{self.base_url}/extract/task/{task_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']
            return None
    
    async def wait_for_completion(self, session: aiohttp.ClientSession,
                                  task_id: str, max_wait: int = 600) -> Optional[Dict]:
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            result = await self.get_task_result(session, task_id)
            
            if result:
                state = result.get('state')
                
                if state == 'done':
                    return result
                elif state == 'failed':
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {result.get('err_msg')}")
                    return None
                elif state == 'running':
                    progress = result.get('extract_progress', {})
                    extracted = progress.get('extracted_pages', 0)
                    total = progress.get('total_pages', 0)
                    if total > 0:
                        print(f"  è¿›åº¦: {extracted}/{total}é¡µ", end='\r')
            
            await asyncio.sleep(5)
        
        print(f"âŒ ä»»åŠ¡è¶…æ—¶")
        return None


class ResultProcessor:
    """ç»“æœå¤„ç†å™¨ - ä¸‹è½½ã€è§£å‹ã€åˆå¹¶"""
    
    @staticmethod
    async def download_and_extract(session: aiohttp.ClientSession,
                                   zip_url: str, output_dir: str) -> Optional[str]:
        """ä¸‹è½½å¹¶è§£å‹ç»“æœ"""
        try:
            # ä¸‹è½½
            async with session.get(zip_url, timeout=aiohttp.ClientTimeout(total=300)) as resp:
                zip_data = await resp.read()
            
            # ä¿å­˜
            zip_path = Path(output_dir) / "result.zip"
            with open(zip_path, 'wb') as f:
                f.write(zip_data)
            
            # è§£å‹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
            
            # åˆ é™¤zip
            zip_path.unlink()
            
            return output_dir
        except Exception as e:
            print(f"âŒ ä¸‹è½½è§£å‹å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def find_markdown(chunk_dir: str) -> Optional[str]:
        """æŸ¥æ‰¾Markdownæ–‡ä»¶"""
        for md_file in Path(chunk_dir).rglob("*.md"):
            return str(md_file)
        return None
    
    @staticmethod
    def merge_results(chunk_dirs: List[str], output_file: str):
        """åˆå¹¶æ‰€æœ‰Markdown"""
        with open(output_file, 'w', encoding='utf-8') as out:
            for i, chunk_dir in enumerate(chunk_dirs, 1):
                if i > 1:
                    out.write("\n\n" + "="*60 + "\n\n")
                
                out.write(f"# åˆ†ç‰‡ {i}\n\n")
                
                md_file = ResultProcessor.find_markdown(chunk_dir)
                if md_file:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        out.write(f.read())
                else:
                    out.write("*å†…å®¹ä¸ºç©º*\n")
        
        print(f"âœ… Markdownåˆå¹¶å®Œæˆ: {output_file}")
    
    @staticmethod
    def merge_images(chunk_dirs: List[str], output_dir: str):
        """åˆå¹¶æ‰€æœ‰å›¾ç‰‡"""
        images_dir = Path(output_dir) / "images"
        images_dir.mkdir(exist_ok=True, parents=True)
        
        count = 0
        for i, chunk_dir in enumerate(chunk_dirs, 1):
            for img in Path(chunk_dir).rglob("*.png"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
            
            for img in Path(chunk_dir).rglob("*.jpg"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
        
        print(f"âœ… å›¾ç‰‡åˆå¹¶å®Œæˆ: {count}ä¸ªæ–‡ä»¶ â†’ {images_dir}")


class MinerUProcessor:
    """MinerU å®Œæ•´å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.client = MinerUClient()
        self.max_workers = max_workers
    
    async def process_file(self, file_path: str, output_dir: str = "./output",
                          **options) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå®Œæ•´æµç¨‹ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            **options: APIå‚æ•°ï¼ˆmodel_version, is_ocrç­‰ï¼‰
        
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
        
        # 1. éªŒè¯æ–‡ä»¶
        is_valid, error, file_info = FileValidator.validate_file(file_path)
        if not is_valid:
            print(f"âŒ {error}")
            return None
        
        print(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡: {file_info['size']/1024/1024:.1f}MB, {file_info['pages']}é¡µ")
        
        # 2. åˆ›å»ºåˆ†ç‰‡é…ç½®
        chunks = SmartChunker.create_chunks_with_ranges(file_info)
        
        # 3. ä¸Šä¼ æ–‡ä»¶ï¼ˆTODO: å®ç°æ–‡ä»¶ä¸Šä¼ ï¼‰
        print("\nâš ï¸  æ³¨æ„: éœ€è¦å…ˆä¸Šä¼ æ–‡ä»¶åˆ°CDN")
        file_url = f"https://example.com/{file_info['name']}"
        
        # 4. å¹¶è¡Œå¤„ç†æ‰€æœ‰åˆ†ç‰‡
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†ç‰‡...")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for chunk in chunks:
                # åˆ›å»ºä»»åŠ¡æ—¶ä¼ å…¥page_ranges
                task_options = {**options}
                if chunk['page_ranges']:
                    task_options['page_ranges'] = chunk['page_ranges']
                
                task = self._process_chunk(session, file_url, chunk, task_options)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
        
        # 5. ä¸‹è½½å¹¶åˆå¹¶ç»“æœ
        success_results = [r for r in results if r]
        
        if not success_results:
            print("âŒ æ‰€æœ‰åˆ†ç‰‡å¤„ç†å¤±è´¥")
            return None
        
        print(f"\nğŸ“¥ ä¸‹è½½å¹¶åˆå¹¶ç»“æœ...")
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        chunk_dirs = []
        async with aiohttp.ClientSession() as session:
            for i, result in enumerate(success_results, 1):
                chunk_dir = output_path / f"chunk_{i}"
                chunk_dir.mkdir(exist_ok=True)
                
                extracted = await ResultProcessor.download_and_extract(
                    session,
                    result['full_zip_url'],
                    str(chunk_dir)
                )
                
                if extracted:
                    chunk_dirs.append(extracted)
        
        # 6. åˆå¹¶
        file_name = Path(file_path).stem
        md_file = output_path / f"{file_name}_merged.md"
        
        ResultProcessor.merge_results(chunk_dirs, str(md_file))
        ResultProcessor.merge_images(chunk_dirs, output_dir)
        
        return {
            'total_chunks': len(chunks),
            'success': len(success_results),
            'failed': len(chunks) - len(success_results),
            'output': {
                'markdown': str(md_file),
                'images': str(output_path / "images")
            }
        }
    
    async def _process_chunk(self, session: aiohttp.ClientSession,
                            file_url: str, chunk: Dict, options: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªåˆ†ç‰‡"""
        chunk_id = chunk.get('chunk_id', 1)
        print(f"  åˆ†ç‰‡{chunk_id}: åˆ›å»ºä»»åŠ¡...")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = await self.client.create_task(session, file_url, **options)
        if not task_id:
            return None
        
        print(f"  åˆ†ç‰‡{chunk_id}: ç­‰å¾…å®Œæˆ...")
        
        # ç­‰å¾…å®Œæˆ
        result = await self.client.wait_for_completion(session, task_id)
        
        if result:
            print(f"  âœ… åˆ†ç‰‡{chunk_id}: å®Œæˆ")
        else:
            print(f"  âŒ åˆ†ç‰‡{chunk_id}: å¤±è´¥")
        
        return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 mineru_production.py <file_path> [options]")
        print("\né€‰é¡¹:")
        print("  --model-version vlm|pipeline|MinerU-HTML")
        print("  --is-ocr true|false")
        print("  --enable-formula true|false")
        print("  --enable-table true|false")
        print("  --language ch|en|...")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    # è§£æé€‰é¡¹
    options = {}
    for i in range(2, len(sys.argv), 2):
        if i + 1 < len(sys.argv):
            key = sys.argv[i].lstrip('--').replace('-', '_')
            value = sys.argv[i + 1]
            
            # è½¬æ¢å¸ƒå°”å€¼
            if value.lower() in ['true', 'false']:
                value = value.lower() == 'true'
            
            options[key] = value
    
    # å¤„ç†æ–‡ä»¶
    processor = MinerUProcessor(max_workers=10)
    result = asyncio.run(processor.process_file(file_path, **options))
    
    if result:
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"  æ€»åˆ†ç‰‡: {result['total_chunks']}")
        print(f"  æˆåŠŸ: {result['success']}")
        print(f"  å¤±è´¥: {result['failed']}")
        print(f"  è¾“å‡º: {result['output']}")
