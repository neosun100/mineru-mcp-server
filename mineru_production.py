#!/usr/bin/env python3
"""
MinerU ç”Ÿäº§çº§å®¢æˆ·ç«¯ - å®Œæ•´è§£å†³æ–¹æ¡ˆ
æ”¯æŒï¼šæ™ºèƒ½æ‹†åˆ†ã€å¹¶è¡Œå¤„ç†ã€å®Œæ•´åˆå¹¶ã€è¿›åº¦ç›‘æ§
ç›´æ¥ä½¿ç”¨APIï¼Œä¸ä¾èµ–å®˜æ–¹SDK
"""
import json
import asyncio
import aiohttp
import random
import time
import zipfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

try:
    from PyPDF2 import PdfReader, PdfWriter
    from pptx import Presentation
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–:")
    print("   uv pip install PyPDF2 python-pptx python-docx aiohttp")
    exit(1)


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨ - æ”¯æŒæœ¬åœ°æ–‡ä»¶å’ŒURL"""
    
    MAX_SIZE = 200 * 1024 * 1024  # 200MB
    MAX_PAGES = 600
    
    SUPPORTED_FORMATS = {
        'pdf': 'application/pdf',
        'doc': 'application/msword',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'ppt': 'application/vnd.ms-powerpoint',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'html': 'text/html'
    }
    
    @staticmethod
    def is_url(path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºURL"""
        return path.startswith(('http://', 'https://'))
    
    @staticmethod
    async def validate_url(session: aiohttp.ClientSession, url: str) -> Tuple[bool, str, Dict]:
        """
        éªŒè¯URL
        
        Returns:
            (is_valid, error_msg, file_info)
        """
        try:
            # HEADè¯·æ±‚è·å–æ–‡ä»¶ä¿¡æ¯
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status != 200:
                    return False, f"URLæ— æ³•è®¿é—®: {resp.status}", {}
                
                # è·å–æ–‡ä»¶å¤§å°
                size = int(resp.headers.get('content-length', 0))
                if size > FileValidator.MAX_SIZE:
                    return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
                
                # ä»URLæˆ–Content-Typeæ¨æ–­æ ¼å¼
                content_type = resp.headers.get('content-type', '')
                format = FileValidator._guess_format_from_url(url, content_type)
                
                if not format:
                    return False, f"æ— æ³•è¯†åˆ«æ–‡ä»¶æ ¼å¼", {}
                
                file_info = {
                    'path': url,
                    'name': Path(url).name or 'document',
                    'size': size,
                    'format': format,
                    'is_url': True,
                    'pages': None,  # URLæ— æ³•é¢„å…ˆè·å–é¡µæ•°
                    'needs_split': False  # ä½¿ç”¨page_rangeså‚æ•°
                }
                
                return True, "", file_info
        
        except Exception as e:
            return False, f"URLéªŒè¯å¤±è´¥: {e}", {}
    
    @staticmethod
    def _guess_format_from_url(url: str, content_type: str) -> Optional[str]:
        """ä»URLå’ŒContent-Typeæ¨æ–­æ ¼å¼"""
        # ä»URLæ‰©å±•åæ¨æ–­
        url_lower = url.lower()
        for ext in FileValidator.SUPPORTED_FORMATS.keys():
            if url_lower.endswith(f'.{ext}'):
                return ext
        
        # ä»Content-Typeæ¨æ–­
        for ext, mime in FileValidator.SUPPORTED_FORMATS.items():
            if mime in content_type:
                return ext
        
        return None
    
    @staticmethod
    def validate_file(file_path: str) -> Tuple[bool, str, Dict]:
        """
        éªŒè¯æœ¬åœ°æ–‡ä»¶
        
        Returns:
            (is_valid, error_msg, file_info)
        """
        path = Path(file_path)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨", {}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = path.stat().st_size
        if size > FileValidator.MAX_SIZE:
            return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
        
        if size == 0:
            return False, "æ–‡ä»¶ä¸ºç©º", {}
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        suffix = path.suffix.lower().lstrip('.')
        if suffix not in FileValidator.SUPPORTED_FORMATS:
            return False, f"ä¸æ”¯æŒçš„æ ¼å¼: {suffix}", {}
        
        # æ£€æŸ¥é¡µæ•°
        pages = FileValidator._get_page_count(file_path, suffix)
        
        file_info = {
            'path': str(path),
            'name': path.name,
            'size': size,
            'format': suffix,
            'is_url': False,
            'pages': pages,
            'needs_split': pages > FileValidator.MAX_PAGES if pages else False
        }
        
        return True, "", file_info
    
    @staticmethod
    def _get_page_count(file_path: str, format: str) -> Optional[int]:
        """è·å–é¡µæ•°"""
        try:
            if format == 'pdf':
                reader = PdfReader(file_path)
                return len(reader.pages)
            elif format in ['pptx', 'ppt']:
                prs = Presentation(file_path)
                return len(prs.slides)
            elif format in ['docx', 'doc']:
                # DOCXé¡µæ•°ä¼°ç®—
                doc = Document(file_path)
                return len(doc.paragraphs) // 5  # ä¼°ç®—ï¼š5æ®µ/é¡µ
        except:
            pass
        return None


class SmartChunker:
    """æ™ºèƒ½æ‹†åˆ†å™¨ - æ”¯æŒpage_rangeså‚æ•°"""
    
    @staticmethod
    def create_chunks_with_ranges(file_info: Dict) -> List[Dict]:
        """
        åˆ›å»ºåˆ†ç‰‡é…ç½®ï¼ˆä½¿ç”¨page_rangeså‚æ•°ï¼‰
        
        Returns:
            [{'file_path': '...', 'page_ranges': '1-600'}, ...]
        """
        pages = file_info['pages']
        if not pages or pages <= FileValidator.MAX_PAGES:
            return [{'file_path': file_info['path'], 'page_ranges': None}]
        
        chunks = []
        chunk_size = FileValidator.MAX_PAGES
        
        for i in range(0, pages, chunk_size):
            start = i + 1  # é¡µç ä»1å¼€å§‹
            end = min(i + chunk_size, pages)
            
            chunks.append({
                'file_path': file_info['path'],
                'page_ranges': f"{start}-{end}",
                'chunk_id': len(chunks) + 1,
                'pages': end - start + 1
            })
        
        print(f"ğŸ“„ æ–‡ä»¶æ‹†åˆ†: {pages}é¡µ â†’ {len(chunks)}ä¸ªåˆ†ç‰‡")
        for chunk in chunks:
            print(f"  åˆ†ç‰‡{chunk['chunk_id']}: é¡µç  {chunk['page_ranges']} ({chunk['pages']}é¡µ)")
        
        return chunks


class MinerUClient:
    """MinerU API å®¢æˆ·ç«¯"""
    
    def __init__(self, tokens_file='all_tokens.json'):
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not Path(tokens_file).is_absolute():
            # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•
            script_dir = Path(__file__).parent
            tokens_file = script_dir / tokens_file
        
        self.tokens_file = str(tokens_file)
        self.tokens = self._load_tokens()
        self.base_url = 'https://mineru.net/api/v4'
        
        if not self.tokens:
            raise ValueError(f"æœªæ‰¾åˆ°Tokenæ–‡ä»¶: {self.tokens_file}")
        
        print(f"âœ… å·²åŠ è½½ {len(self.tokens)} ä¸ªè´¦æˆ·")
    
    def _load_tokens(self) -> Dict:
        """åŠ è½½Token"""
        try:
            with open(self.tokens_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _get_random_token(self) -> str:
        """éšæœºé€‰æ‹©Token"""
        email = random.choice(list(self.tokens.keys()))
        return self.tokens[email]['token']
    
    async def create_task(self, session: aiohttp.ClientSession, 
                         file_url: str, **options) -> Optional[str]:
        """
        åˆ›å»ºè§£æä»»åŠ¡
        
        Args:
            file_url: æ–‡ä»¶URL
            **options: å…¶ä»–å‚æ•°ï¼ˆpage_ranges, model_versionç­‰ï¼‰
        
        Returns:
            task_id
        """
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        data = {'url': file_url, **options}
        
        async with session.post(
            f"{self.base_url}/extract/task",
            headers=headers,
            json=data,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']['task_id']
            else:
                print(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {result.get('msg')}")
                return None
    
    async def get_task_result(self, session: aiohttp.ClientSession, 
                             task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}'
        }
        
        async with session.get(
            f"{self.base_url}/extract/task/{task_id}",
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as resp:
            result = await resp.json()
            
            if result['code'] == 0:
                return result['data']
            return None
    
    async def wait_for_completion(self, session: aiohttp.ClientSession,
                                  task_id: str, max_wait: int = 600) -> Optional[Dict]:
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            result = await self.get_task_result(session, task_id)
            
            if result:
                state = result.get('state')
                
                if state == 'done':
                    return result
                elif state == 'failed':
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {result.get('err_msg')}")
                    return None
                elif state == 'running':
                    progress = result.get('extract_progress', {})
                    extracted = progress.get('extracted_pages', 0)
                    total = progress.get('total_pages', 0)
                    if total > 0:
                        print(f"  è¿›åº¦: {extracted}/{total}é¡µ", end='\r')
            
            await asyncio.sleep(5)
        
        print(f"âŒ ä»»åŠ¡è¶…æ—¶")
        return None


class ResultProcessor:
    """ç»“æœå¤„ç†å™¨ - ä¸‹è½½ã€è§£å‹ã€åˆå¹¶"""
    
    @staticmethod
    async def download_and_extract(session: aiohttp.ClientSession,
                                   zip_url: str, output_dir: str) -> Optional[str]:
        """ä¸‹è½½å¹¶è§£å‹ç»“æœ"""
        try:
            # ä¸‹è½½
            async with session.get(zip_url, timeout=aiohttp.ClientTimeout(total=300)) as resp:
                zip_data = await resp.read()
            
            # ä¿å­˜
            zip_path = Path(output_dir) / "result.zip"
            with open(zip_path, 'wb') as f:
                f.write(zip_data)
            
            # è§£å‹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
            
            # åˆ é™¤zip
            zip_path.unlink()
            
            return output_dir
        except Exception as e:
            print(f"âŒ ä¸‹è½½è§£å‹å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def find_markdown(chunk_dir: str) -> Optional[str]:
        """æŸ¥æ‰¾Markdownæ–‡ä»¶"""
        for md_file in Path(chunk_dir).rglob("*.md"):
            return str(md_file)
        return None
    
    @staticmethod
    def merge_results(chunk_dirs: List[str], output_file: str):
        """åˆå¹¶æ‰€æœ‰Markdown"""
        with open(output_file, 'w', encoding='utf-8') as out:
            for i, chunk_dir in enumerate(chunk_dirs, 1):
                if i > 1:
                    out.write("\n\n" + "="*60 + "\n\n")
                
                out.write(f"# åˆ†ç‰‡ {i}\n\n")
                
                md_file = ResultProcessor.find_markdown(chunk_dir)
                if md_file:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        out.write(f.read())
                else:
                    out.write("*å†…å®¹ä¸ºç©º*\n")
        
        print(f"âœ… Markdownåˆå¹¶å®Œæˆ: {output_file}")
    
    @staticmethod
    def merge_images(chunk_dirs: List[str], output_dir: str):
        """åˆå¹¶æ‰€æœ‰å›¾ç‰‡"""
        images_dir = Path(output_dir) / "images"
        images_dir.mkdir(exist_ok=True, parents=True)
        
        count = 0
        for i, chunk_dir in enumerate(chunk_dirs, 1):
            for img in Path(chunk_dir).rglob("*.png"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
            
            for img in Path(chunk_dir).rglob("*.jpg"):
                new_name = f"chunk_{i}_{img.name}"
                shutil.copy(img, images_dir / new_name)
                count += 1
        
        print(f"âœ… å›¾ç‰‡åˆå¹¶å®Œæˆ: {count}ä¸ªæ–‡ä»¶ â†’ {images_dir}")


class MinerUProcessor:
    """MinerU å®Œæ•´å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.client = MinerUClient()
        self.max_workers = max_workers
    
    async def process_file(self, file_path: str, output_dir: str = "./output",
                          **options) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå®Œæ•´æµç¨‹ï¼‰- æ”¯æŒæœ¬åœ°æ–‡ä»¶å’ŒURL
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„æˆ–URL
            output_dir: è¾“å‡ºç›®å½•
            **options: APIå‚æ•°ï¼ˆmodel_version, is_ocrç­‰ï¼‰
        
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"\nğŸ“„ å¤„ç†: {file_path}")
        
        # 1. éªŒè¯æ–‡ä»¶æˆ–URL
        if FileValidator.is_url(file_path):
            print("ğŸŒ æ£€æµ‹åˆ°URLï¼ŒéªŒè¯ä¸­...")
            async with aiohttp.ClientSession() as session:
                is_valid, error, file_info = await FileValidator.validate_url(session, file_path)
        else:
            print("ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ï¼ŒéªŒè¯ä¸­...")
            is_valid, error, file_info = FileValidator.validate_file(file_path)
        
        if not is_valid:
            print(f"âŒ {error}")
            return None
        
        print(f"âœ… éªŒè¯é€šè¿‡: {file_info['format'].upper()}, {file_info['size']/1024/1024:.1f}MB")
        
        # 2. åˆ›å»ºåˆ†ç‰‡é…ç½®
        if file_info['is_url']:
            # URLç›´æ¥ä½¿ç”¨ï¼Œé€šè¿‡page_rangeså‚æ•°æ‹†åˆ†
            chunks = SmartChunker.create_chunks_with_ranges(file_info)
        else:
            # æœ¬åœ°æ–‡ä»¶ä¹Ÿä½¿ç”¨page_ranges
            chunks = SmartChunker.create_chunks_with_ranges(file_info)
        
        # 3. å¤„ç†æ‰€æœ‰åˆ†ç‰‡
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†ç‰‡...")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for chunk in chunks:
                # åˆ›å»ºä»»åŠ¡æ—¶ä¼ å…¥page_ranges
                task_options = {**options}
                if chunk['page_ranges']:
                    task_options['page_ranges'] = chunk['page_ranges']
                
                # ä½¿ç”¨åŸå§‹è·¯å¾„ï¼ˆURLæˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
                file_url = file_info['path']
                
                task = self._process_chunk(session, file_url, chunk, task_options)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
        
        # 4. ä¸‹è½½å¹¶åˆå¹¶ç»“æœ
        success_results = [r for r in results if r]
        
        if not success_results:
            print("âŒ æ‰€æœ‰åˆ†ç‰‡å¤„ç†å¤±è´¥")
            return None
        
        print(f"\nğŸ“¥ ä¸‹è½½å¹¶åˆå¹¶ç»“æœ...")
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        chunk_dirs = []
        async with aiohttp.ClientSession() as session:
            for i, result in enumerate(success_results, 1):
                chunk_dir = output_path / f"chunk_{i}"
                chunk_dir.mkdir(exist_ok=True)
                
                extracted = await ResultProcessor.download_and_extract(
                    session,
                    result['full_zip_url'],
                    str(chunk_dir)
                )
                
                if extracted:
                    chunk_dirs.append(extracted)
        
        # 5. åˆå¹¶
        file_name = file_info['name'].rsplit('.', 1)[0]  # å»é™¤æ‰©å±•å
        md_file = output_path / f"{file_name}_merged.md"
        
        ResultProcessor.merge_results(chunk_dirs, str(md_file))
        ResultProcessor.merge_images(chunk_dirs, output_dir)
        
        return {
            'source': file_path,
            'source_type': 'url' if file_info['is_url'] else 'file',
            'total_chunks': len(chunks),
            'success': len(success_results),
            'failed': len(chunks) - len(success_results),
            'output': {
                'markdown': str(md_file),
                'images': str(output_path / "images")
            }
        }
    
    async def _process_chunk(self, session: aiohttp.ClientSession,
                            file_url: str, chunk: Dict, options: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªåˆ†ç‰‡"""
        chunk_id = chunk.get('chunk_id', 1)
        print(f"  åˆ†ç‰‡{chunk_id}: åˆ›å»ºä»»åŠ¡...")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = await self.client.create_task(session, file_url, **options)
        if not task_id:
            return None
        
        print(f"  åˆ†ç‰‡{chunk_id}: ç­‰å¾…å®Œæˆ...")
        
        # ç­‰å¾…å®Œæˆ
        result = await self.client.wait_for_completion(session, task_id)
        
        if result:
            print(f"  âœ… åˆ†ç‰‡{chunk_id}: å®Œæˆ")
        else:
            print(f"  âŒ åˆ†ç‰‡{chunk_id}: å¤±è´¥")
        
        return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 mineru_production.py <file_path> [options]")
        print("\né€‰é¡¹:")
        print("  --model-version vlm|pipeline|MinerU-HTML")
        print("  --is-ocr true|false")
        print("  --enable-formula true|false")
        print("  --enable-table true|false")
        print("  --language ch|en|...")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    # è§£æé€‰é¡¹
    options = {}
    for i in range(2, len(sys.argv), 2):
        if i + 1 < len(sys.argv):
            key = sys.argv[i].lstrip('--').replace('-', '_')
            value = sys.argv[i + 1]
            
            # è½¬æ¢å¸ƒå°”å€¼
            if value.lower() in ['true', 'false']:
                value = value.lower() == 'true'
            
            options[key] = value
    
    # å¤„ç†æ–‡ä»¶
    processor = MinerUProcessor(max_workers=10)
    result = asyncio.run(processor.process_file(file_path, **options))
    
    if result:
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"  æ€»åˆ†ç‰‡: {result['total_chunks']}")
        print(f"  æˆåŠŸ: {result['success']}")
        print(f"  å¤±è´¥: {result['failed']}")
        print(f"  è¾“å‡º: {result['output']}")
