# MinerU API é¡µç èŒƒå›´å’Œåˆå¹¶ç­–ç•¥è¯´æ˜

## ğŸ“‹ å…³äºé¡µç èŒƒå›´å‚æ•°

### APIæ˜¯å¦æ”¯æŒé¡µç èŒƒå›´ï¼Ÿ

ç»è¿‡æŸ¥é˜…MinerUå®˜æ–¹æ–‡æ¡£å’ŒAPIæ–‡æ¡£ï¼Œ**ç›®å‰APIä¸æ”¯æŒç›´æ¥æŒ‡å®šé¡µç èŒƒå›´**ï¼ˆå¦‚ `start_page`, `end_page`ï¼‰ã€‚

### ä¸ºä»€ä¹ˆä¸æ”¯æŒï¼Ÿ

MinerU APIçš„è®¾è®¡ç†å¿µæ˜¯ï¼š
1. **å®Œæ•´æ€§ä¼˜å…ˆ**ï¼šç¡®ä¿æ–‡æ¡£ç»“æ„å®Œæ•´æ€§
2. **ä¸Šä¸‹æ–‡å…³è”**ï¼šé¿å…ç ´åæ–‡æ¡£çš„ä¸Šä¸‹æ–‡å…³ç³»
3. **ç®€åŒ–æ¥å£**ï¼šä¿æŒAPIç®€æ´æ˜“ç”¨

### æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆ

æ—¢ç„¶APIä¸æ”¯æŒé¡µç èŒƒå›´ï¼Œæˆ‘ä»¬é‡‡ç”¨**å®¢æˆ·ç«¯æ‹†åˆ†**ç­–ç•¥ï¼š

```python
# åœ¨å®¢æˆ·ç«¯æ‹†åˆ†PDF
chunks = FileChunker.split_pdf("large.pdf", output_dir="./chunks")

# æ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹å¤„ç†
for chunk in chunks:
    process(chunk)

# åˆå¹¶ç»“æœ
merge_results(results)
```

**ä¼˜åŠ¿ï¼š**
- âœ… å®Œå…¨æ§åˆ¶æ‹†åˆ†é€»è¾‘
- âœ… å¯ä»¥è‡ªå®šä¹‰æ‹†åˆ†ç­–ç•¥
- âœ… æ”¯æŒæ‰€æœ‰æ–‡ä»¶æ ¼å¼ï¼ˆPDF/PPTX/DOCXï¼‰
- âœ… ä¸ä¾èµ–APIé™åˆ¶

## ğŸ”— å®Œæ•´åˆå¹¶ç­–ç•¥

### MinerU APIè¿”å›ç»“æ„

```json
{
  "task_id": "xxx",
  "state": "done",
  "full_zip_url": "https://cdn.../result.zip",
  "md_url": "https://cdn.../result.md",
  "md_content_url": "https://cdn.../content.md",
  "layout_tree_url": "https://cdn.../layout.json"
}
```

### å‹ç¼©åŒ…å†…å®¹ç»“æ„

```
result.zip
â”œâ”€â”€ auto/
â”‚   â”œâ”€â”€ xxx.md          # Markdownæ–‡ä»¶
â”‚   â”œâ”€â”€ images/         # å›¾ç‰‡ç›®å½•
â”‚   â”‚   â”œâ”€â”€ img_001.png
â”‚   â”‚   â”œâ”€â”€ img_002.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ layout.json     # å¸ƒå±€ä¿¡æ¯
```

### æˆ‘ä»¬çš„åˆå¹¶æµç¨‹

#### 1. ä¸‹è½½å¹¶è§£å‹

```python
async def download_and_extract_results(results, output_dir):
    """
    ä¸‹è½½æ‰€æœ‰åˆ†ç‰‡çš„å‹ç¼©åŒ…å¹¶è§£å‹
    
    ç»“æ„:
    output_dir/
    â”œâ”€â”€ chunk_1/
    â”‚   â”œâ”€â”€ auto/
    â”‚   â”‚   â”œâ”€â”€ xxx.md
    â”‚   â”‚   â””â”€â”€ images/
    â”œâ”€â”€ chunk_2/
    â”‚   â”œâ”€â”€ auto/
    â”‚   â”‚   â”œâ”€â”€ xxx.md
    â”‚   â”‚   â””â”€â”€ images/
    â””â”€â”€ ...
    """
```

#### 2. åˆå¹¶Markdown

```python
def merge_markdown_files(extracted_results, output_file):
    """
    åˆå¹¶æ‰€æœ‰Markdownå†…å®¹
    
    ç­–ç•¥:
    - æŒ‰åˆ†ç‰‡é¡ºåºåˆå¹¶
    - åˆ†ç‰‡ä¹‹é—´æ·»åŠ åˆ†éš”ç¬¦
    - ä¿ç•™åŸå§‹æ ¼å¼
    - è‡ªåŠ¨å¤„ç†å›¾ç‰‡å¼•ç”¨
    """
    with open(output_file, 'w') as f:
        for i, result in enumerate(extracted_results, 1):
            if i > 1:
                f.write("\n\n" + "="*60 + "\n\n")
            
            f.write(f"# åˆ†ç‰‡ {i}\n\n")
            f.write(result['md_content'])
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```markdown
# åˆ†ç‰‡ 1

## ç¬¬ä¸€ç« 
å†…å®¹...

============================================================

# åˆ†ç‰‡ 2

## ç¬¬äºŒç« 
å†…å®¹...
```

#### 3. åˆå¹¶å›¾ç‰‡

```python
def merge_images(extracted_results, output_dir):
    """
    åˆå¹¶æ‰€æœ‰å›¾ç‰‡åˆ°ç»Ÿä¸€ç›®å½•
    
    ç­–ç•¥:
    - æ‰€æœ‰å›¾ç‰‡æ”¾åœ¨ images/ ç›®å½•
    - æ–‡ä»¶åæ·»åŠ åˆ†ç‰‡å‰ç¼€é¿å…å†²çª
    - ä¿ç•™åŸå§‹æ–‡ä»¶å
    """
    images_dir = Path(output_dir) / "images"
    
    for result in extracted_results:
        for img in find_images(result['chunk_dir']):
            new_name = f"chunk_{result['chunk_id']}_{img.name}"
            copy(img, images_dir / new_name)
```

**è¾“å‡ºç»“æ„ï¼š**
```
output/
â””â”€â”€ images/
    â”œâ”€â”€ chunk_1_img_001.png
    â”œâ”€â”€ chunk_1_img_002.png
    â”œâ”€â”€ chunk_2_img_001.png
    â”œâ”€â”€ chunk_2_img_002.png
    â””â”€â”€ ...
```

#### 4. åˆå¹¶å…ƒæ•°æ®

```python
def merge_json_metadata(extracted_results, output_file):
    """
    åˆå¹¶æ‰€æœ‰å…ƒæ•°æ®
    
    åŒ…å«:
    - åˆ†ç‰‡ä¿¡æ¯
    - ä¸‹è½½URL
    - æ—¶é—´æˆ³
    - æ–‡ä»¶è·¯å¾„
    """
    merged = {
        'total_chunks': len(extracted_results),
        'merged_at': datetime.now().isoformat(),
        'chunks': [...]
    }
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```json
{
  "total_chunks": 2,
  "merged_at": "2026-01-25T15:45:00",
  "chunks": [
    {
      "chunk_id": 1,
      "chunk_dir": "./output/chunk_1",
      "has_content": true,
      "urls": {
        "full_zip_url": "https://...",
        "md_url": "https://..."
      }
    },
    {
      "chunk_id": 2,
      ...
    }
  ]
}
```

### æœ€ç»ˆè¾“å‡ºç»“æ„

```
output/
â”œâ”€â”€ large_document_merged.md      # åˆå¹¶åçš„Markdown
â”œâ”€â”€ large_document_metadata.json  # å…ƒæ•°æ®
â”œâ”€â”€ images/                       # æ‰€æœ‰å›¾ç‰‡
â”‚   â”œâ”€â”€ chunk_1_img_001.png
â”‚   â”œâ”€â”€ chunk_1_img_002.png
â”‚   â”œâ”€â”€ chunk_2_img_001.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chunk_1/                      # åŸå§‹åˆ†ç‰‡1
â”‚   â””â”€â”€ auto/
â”‚       â”œâ”€â”€ xxx.md
â”‚       â””â”€â”€ images/
â””â”€â”€ chunk_2/                      # åŸå§‹åˆ†ç‰‡2
    â””â”€â”€ auto/
        â”œâ”€â”€ xxx.md
        â””â”€â”€ images/
```

## ğŸ¯ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from mineru_batch_processor import MinerUBatchProcessor

# åˆå§‹åŒ–
processor = MinerUBatchProcessor(max_workers=10)

# å¤„ç†å¤§æ–‡ä»¶
result = processor.process_large_file(
    file_path="large_1000_pages.pdf",
    output_dir="./output"
)

# ç»“æœ
print(f"æ€»åˆ†ç‰‡: {result['total_chunks']}")
print(f"æˆåŠŸ: {result['success']}")
print(f"å¤±è´¥: {result['failed']}")
print(f"è¾“å‡ºæ–‡ä»¶:")
print(f"  Markdown: {result['output_files']['markdown']}")
print(f"  å›¾ç‰‡ç›®å½•: {result['output_files']['images']}")
print(f"  å…ƒæ•°æ®: {result['output_files']['metadata']}")
```

## ğŸ’¡ å…³é”®ä¼˜åŠ¿

### 1. å®Œæ•´æ€§ä¿è¯
- âœ… ä¸‹è½½å®Œæ•´å‹ç¼©åŒ…
- âœ… ä¿ç•™æ‰€æœ‰å›¾ç‰‡
- âœ… ä¿ç•™å¸ƒå±€ä¿¡æ¯
- âœ… ä¿ç•™å…ƒæ•°æ®

### 2. æ™ºèƒ½åˆå¹¶
- âœ… MarkdownæŒ‰é¡ºåºåˆå¹¶
- âœ… å›¾ç‰‡ç»Ÿä¸€ç®¡ç†
- âœ… é¿å…æ–‡ä»¶åå†²çª
- âœ… ä¿ç•™åŸå§‹æ–‡ä»¶

### 3. å¯è¿½æº¯æ€§
- âœ… ä¿ç•™åŸå§‹åˆ†ç‰‡
- âœ… è®°å½•åˆå¹¶æ—¶é—´
- âœ… è®°å½•ä¸‹è½½URL
- âœ… å®Œæ•´å…ƒæ•°æ®

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å›¾ç‰‡å¼•ç”¨æ›´æ–°

åˆå¹¶åçš„Markdownä¸­çš„å›¾ç‰‡å¼•ç”¨éœ€è¦æ›´æ–°ï¼š

```markdown
<!-- åŸå§‹ -->
![](images/img_001.png)

<!-- æ›´æ–°å -->
![](images/chunk_1_img_001.png)
```

**TODO**: è‡ªåŠ¨æ›´æ–°å›¾ç‰‡å¼•ç”¨è·¯å¾„

### 2. ç£ç›˜ç©ºé—´

å¤„ç†å¤§æ–‡ä»¶éœ€è¦è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼š
- åŸå§‹æ–‡ä»¶
- æ‹†åˆ†åçš„åˆ†ç‰‡
- ä¸‹è½½çš„å‹ç¼©åŒ…
- è§£å‹åçš„æ–‡ä»¶
- åˆå¹¶åçš„æ–‡ä»¶

**å»ºè®®**: é¢„ç•™ 5x æ–‡ä»¶å¤§å°çš„ç©ºé—´

### 3. ç½‘ç»œç¨³å®šæ€§

ä¸‹è½½å¤§é‡å‹ç¼©åŒ…éœ€è¦ç¨³å®šçš„ç½‘ç»œï¼š
- ä½¿ç”¨å¼‚æ­¥ä¸‹è½½æé«˜æ•ˆç‡
- æ·»åŠ é‡è¯•æœºåˆ¶
- æ˜¾ç¤ºä¸‹è½½è¿›åº¦

## ğŸ”§ æœªæ¥ä¼˜åŒ–

### 1. æµå¼åˆå¹¶

ä¸ä¸‹è½½å®Œæ•´å‹ç¼©åŒ…ï¼Œç›´æ¥æµå¼åˆå¹¶ï¼š
```python
# TODO
async def stream_merge(results):
    async for chunk in download_stream(results):
        merge_chunk(chunk)
```

### 2. å¢é‡åˆå¹¶

æ”¯æŒå¢é‡æ·»åŠ æ–°åˆ†ç‰‡ï¼š
```python
# TODO
def incremental_merge(existing_result, new_chunks):
    append_to_markdown(new_chunks)
    update_metadata(new_chunks)
```

### 3. æ™ºèƒ½å»é‡

æ£€æµ‹å¹¶å»é™¤é‡å¤å†…å®¹ï¼š
```python
# TODO
def deduplicate_content(merged_content):
    # æ£€æµ‹é‡å¤æ®µè½
    # æ™ºèƒ½å»é‡
    pass
```

---

**âœ… å®Œæ•´çš„åˆå¹¶ç­–ç•¥ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ€§å’Œå¯è¿½æº¯æ€§ï¼**
