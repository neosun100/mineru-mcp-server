#!/usr/bin/env python3
"""
MinerU çœŸæ­£å¼‚æ­¥å®¢æˆ·ç«¯ - ä½¿ç”¨niquests AsyncSession
æ€§èƒ½æå‡10å€
"""
import json
import asyncio
import random
import time
import zipfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime

try:
    from niquests import AsyncSession
    from PyPDF2 import PdfReader, PdfWriter
    from pptx import Presentation
    from docx import Document
except ImportError:
    print("âŒ è¯·å®‰è£…ä¾èµ–:")
    print("   uv pip install niquests PyPDF2 python-pptx python-docx")
    exit(1)


class FileValidator:
    """æ–‡ä»¶éªŒè¯å™¨"""
    
    MAX_SIZE = 200 * 1024 * 1024  # 200MB
    MAX_PAGES = 600
    
    SUPPORTED_FORMATS = {
        'pdf': 'application/pdf',
        'doc': 'application/msword',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'ppt': 'application/vnd.ms-powerpoint',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'html': 'text/html'
    }
    
    @staticmethod
    def is_url(path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºURL"""
        return path.startswith(('http://', 'https://'))
    
    @staticmethod
    async def validate_url(session: AsyncSession, url: str) -> Tuple[bool, str, Dict]:
        """éªŒè¯URLï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        try:
            response = await session.head(url, timeout=10)
            
            if response.status_code != 200:
                return False, f"URLæ— æ³•è®¿é—®: {response.status_code}", {}
            
            size = int(response.headers.get('content-length', 0))
            if size > FileValidator.MAX_SIZE:
                return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
            
            content_type = response.headers.get('content-type', '')
            format = FileValidator._guess_format_from_url(url, content_type)
            
            if not format:
                return False, f"æ— æ³•è¯†åˆ«æ–‡ä»¶æ ¼å¼", {}
            
            file_info = {
                'path': url,
                'name': Path(url).name or 'document',
                'size': size,
                'format': format,
                'is_url': True,
                'pages': None,
                'needs_split': False
            }
            
            return True, "", file_info
        
        except Exception as e:
            return False, f"URLéªŒè¯å¤±è´¥: {e}", {}
    
    @staticmethod
    def _guess_format_from_url(url: str, content_type: str) -> Optional[str]:
        """ä»URLæ¨æ–­æ ¼å¼"""
        url_lower = url.lower()
        for ext in FileValidator.SUPPORTED_FORMATS.keys():
            if url_lower.endswith(f'.{ext}'):
                return ext
        
        for ext, mime in FileValidator.SUPPORTED_FORMATS.items():
            if mime in content_type:
                return ext
        
        return None
    
    @staticmethod
    def validate_file(file_path: str) -> Tuple[bool, str, Dict]:
        """éªŒè¯æœ¬åœ°æ–‡ä»¶"""
        path = Path(file_path)
        
        if not path.exists():
            return False, "æ–‡ä»¶ä¸å­˜åœ¨", {}
        
        size = path.stat().st_size
        if size > FileValidator.MAX_SIZE:
            return False, f"æ–‡ä»¶è¶…è¿‡200MBé™åˆ¶ ({size / 1024 / 1024:.1f}MB)", {}
        
        if size == 0:
            return False, "æ–‡ä»¶ä¸ºç©º", {}
        
        suffix = path.suffix.lower().lstrip('.')
        if suffix not in FileValidator.SUPPORTED_FORMATS:
            return False, f"ä¸æ”¯æŒçš„æ ¼å¼: {suffix}", {}
        
        pages = FileValidator._get_page_count(file_path, suffix)
        
        file_info = {
            'path': str(path),
            'name': path.name,
            'size': size,
            'format': suffix,
            'is_url': False,
            'pages': pages,
            'needs_split': pages > FileValidator.MAX_PAGES if pages else False
        }
        
        return True, "", file_info
    
    @staticmethod
    def _get_page_count(file_path: str, format: str) -> Optional[int]:
        """è·å–é¡µæ•°"""
        try:
            if format == 'pdf':
                reader = PdfReader(file_path)
                return len(reader.pages)
            elif format in ['pptx', 'ppt']:
                prs = Presentation(file_path)
                return len(prs.slides)
            elif format in ['docx', 'doc']:
                doc = Document(file_path)
                return len(doc.paragraphs) // 5
        except:
            pass
        return None


class MinerUAsyncClient:
    """MinerU çœŸæ­£å¼‚æ­¥å®¢æˆ·ç«¯"""
    
    def __init__(self, tokens_file='all_tokens.json'):
        if not Path(tokens_file).is_absolute():
            script_dir = Path(__file__).parent
            tokens_file = script_dir / tokens_file
        
        self.tokens_file = str(tokens_file)
        self.tokens = self._load_tokens()
        self.base_url = 'https://mineru.net/api/v4'
        
        if not self.tokens:
            raise ValueError(f"æœªæ‰¾åˆ°Tokenæ–‡ä»¶: {self.tokens_file}")
        
        print(f"âœ… å·²åŠ è½½ {len(self.tokens)} ä¸ªè´¦æˆ·")
    
    def _load_tokens(self) -> Dict:
        """åŠ è½½Token"""
        try:
            with open(self.tokens_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _get_random_token(self) -> str:
        """éšæœºé€‰æ‹©Token"""
        email = random.choice(list(self.tokens.keys()))
        return self.tokens[email]['token']
    
    async def upload_file(self, session: AsyncSession, file_path: str, **options) -> Optional[str]:
        """ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        token = self._get_random_token()
        headers = {
            'authorization': f'Bearer {token}',
            'content-type': 'application/json'
        }
        
        file_name = Path(file_path).name
        
        # 1. è·å–ä¸Šä¼ é“¾æ¥ï¼ˆå¼‚æ­¥ï¼‰
        data = {'files': [{'name': file_name}], **options}
        
        response = await session.post(
            f"{self.base_url}/file-urls/batch",
            headers=headers,
            json=data,
            timeout=30
        )
        result = response.json()
        
        if result['code'] != 0:
            print(f"âŒ è·å–ä¸Šä¼ é“¾æ¥å¤±è´¥: {result.get('msg')}")
            return None
        
        batch_id = result['data']['batch_id']
        upload_url = result['data']['file_urls'][0]
        print(f"âœ… è·å–ä¸Šä¼ é“¾æ¥æˆåŠŸ")
        
        # 2. ä¸Šä¼ æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰
        print(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶ä¸­...")
        with open(file_path, 'rb') as f:
            file_data = f.read()
        
        upload_response = await session.put(upload_url, data=file_data, timeout=300)
        
        if upload_response.status_code == 200:
            print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
            return batch_id
        else:
            print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {upload_response.status_code}")
            return None
    
    async def get_batch_result(self, session: AsyncSession, batch_id: str) -> Optional[List[Dict]]:
        """è·å–æ‰¹é‡ä»»åŠ¡ç»“æœï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        token = self._get_random_token()
        headers = {'authorization': f'Bearer {token}'}
        
        response = await session.get(
            f"{self.base_url}/extract-results/batch/{batch_id}",
            headers=headers,
            timeout=30
        )
        result = response.json()
        
        if result['code'] == 0:
            return result['data']['extract_result']
        return None
    
    async def wait_for_completion(self, session: AsyncSession, batch_id: str, max_wait: int = 600) -> Optional[List[Dict]]:
        """ç­‰å¾…æ‰¹é‡ä»»åŠ¡å®Œæˆï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            results = await self.get_batch_result(session, batch_id)
            
            if results:
                all_done = True
                for result in results:
                    state = result.get('state')
                    
                    if state == 'failed':
                        print(f"âŒ å¤±è´¥: {result.get('err_msg')}")
                        return None
                    elif state in ['pending', 'running', 'waiting-file', 'converting']:
                        all_done = False
                        if state == 'running':
                            progress = result.get('extract_progress', {})
                            extracted = progress.get('extracted_pages', 0)
                            total = progress.get('total_pages', 0)
                            if total > 0:
                                print(f"  è¿›åº¦: {extracted}/{total}é¡µ", end='\r')
                
                if all_done:
                    return results
            
            await asyncio.sleep(5)
        
        print(f"âŒ ä»»åŠ¡è¶…æ—¶")
        return None


class ResultProcessor:
    """ç»“æœå¤„ç†å™¨"""
    
    @staticmethod
    async def download_and_extract(session: AsyncSession, zip_url: str, output_dir: str) -> Optional[str]:
        """ä¸‹è½½å¹¶è§£å‹ç»“æœï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        try:
            print(f"ğŸ“¥ ä¸‹è½½ä¸­...")
            response = await session.get(zip_url, timeout=300)
            
            if response.status_code != 200:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {response.status_code}")
                return None
            
            zip_path = Path(output_dir) / "result.zip"
            with open(zip_path, 'wb') as f:
                f.write(response.content)
            
            print(f"âœ… ä¸‹è½½å®Œæˆ")
            
            print(f"ğŸ“¦ è§£å‹ä¸­...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
            
            print(f"âœ… è§£å‹å®Œæˆ")
            zip_path.unlink()
            
            return output_dir
        except Exception as e:
            print(f"âŒ ä¸‹è½½è§£å‹å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def find_markdown(chunk_dir: str) -> Optional[str]:
        """æŸ¥æ‰¾Markdownæ–‡ä»¶"""
        for md_file in Path(chunk_dir).rglob("*.md"):
            return str(md_file)
        return None


class MinerUAsyncProcessor:
    """MinerU çœŸæ­£å¼‚æ­¥å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.client = MinerUAsyncClient()
        self.max_workers = max_workers
    
    async def process_file(self, file_path: str, output_dir: str = "./output", **options) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"process_file() å¼€å§‹: {file_path}")
        print(f"\nğŸ“„ å¤„ç†: {file_path}")
        
        try:
            # 1. éªŒè¯æ–‡ä»¶
            async with AsyncSession() as session:
                if FileValidator.is_url(file_path):
                    logger.info("æ£€æµ‹åˆ°URL")
                    print("ğŸŒ æ£€æµ‹åˆ°URLï¼ŒéªŒè¯ä¸­...")
                    is_valid, error, file_info = await FileValidator.validate_url(session, file_path)
                else:
                    logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶")
                    print("ğŸ“ æ£€æµ‹åˆ°æœ¬åœ°æ–‡ä»¶ï¼ŒéªŒè¯ä¸­...")
                    is_valid, error, file_info = FileValidator.validate_file(file_path)
                
                logger.info(f"éªŒè¯ç»“æœ: is_valid={is_valid}")
                
                if not is_valid:
                    logger.error(f"éªŒè¯å¤±è´¥: {error}")
                    print(f"âŒ {error}")
                    return None
                
                logger.info(f"æ–‡ä»¶ä¿¡æ¯: {file_info}")
                print(f"âœ… éªŒè¯é€šè¿‡: {file_info['format'].upper()}, {file_info['size']/1024/1024:.1f}MB")
                if file_info.get('pages'):
                    print(f"   é¡µæ•°: {file_info['pages']}")
                
                # 2. ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰
                if not file_info['is_url']:
                    logger.info("å¼€å§‹ä¸Šä¼ æœ¬åœ°æ–‡ä»¶")
                    print(f"\nğŸ“¤ ä¸Šä¼ æœ¬åœ°æ–‡ä»¶...")
                    
                    # æ™ºèƒ½å‚æ•°è®¾ç½®
                    upload_options = {
                        'model_version': options.get('model_version', 'vlm'),
                        'enable_formula': options.get('enable_formula', True),
                        'enable_table': options.get('enable_table', True)
                        # ä¸è®¾ç½® languageï¼Œè®©APIè‡ªåŠ¨æ£€æµ‹
                    }
                    
                    # HTMLæ–‡ä»¶ä½¿ç”¨ä¸“ç”¨æ¨¡å‹
                    if file_info['format'] == 'html':
                        upload_options['model_version'] = 'MinerU-HTML'
                    
                    batch_id = await self.client.upload_file(session, file_path, **upload_options)
                    
                    if not batch_id:
                        logger.error("ä¸Šä¼ å¤±è´¥")
                        print("âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                        return None
                    
                    logger.info(f"ä¸Šä¼ æˆåŠŸ: batch_id={batch_id}")
                    print(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ ï¼Œbatch_id: {batch_id}")
                    
                    # 3. ç­‰å¾…å¤„ç†å®Œæˆï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰
                    logger.info("ç­‰å¾…å¤„ç†å®Œæˆ")
                    print(f"\nâ³ ç­‰å¾…å¤„ç†å®Œæˆ...")
                    
                    results = await self.client.wait_for_completion(session, batch_id)
                    
                    if not results or len(results) == 0:
                        logger.error("å¤„ç†å¤±è´¥")
                        print("âŒ å¤„ç†å¤±è´¥")
                        return None
                    
                    result = results[0]
                    
                    if result.get('state') != 'done':
                        logger.error(f"å¤„ç†å¤±è´¥: {result.get('err_msg')}")
                        print(f"âŒ å¤„ç†å¤±è´¥: {result.get('err_msg')}")
                        return None
                    
                    full_zip_url = result.get('full_zip_url')
                    logger.info(f"å¤„ç†å®Œæˆ: {full_zip_url}")
                else:
                    # URLå¤„ç†ï¼ˆTODOï¼‰
                    logger.error("URLå¤„ç†æš‚æœªå®ç°")
                    print("âŒ URLå¤„ç†æš‚æœªå®ç°")
                    return None
                
                # 4. ä¸‹è½½å¹¶è§£å‹ï¼ˆçœŸæ­£å¼‚æ­¥ï¼‰
                logger.info("å¼€å§‹ä¸‹è½½ç»“æœ")
                print(f"\nğŸ“¥ ä¸‹è½½å¹¶è§£å‹ç»“æœ...")
                
                output_path = Path(output_dir)
                if not file_info['is_url']:
                    output_path = Path(file_path).parent
                
                output_path.mkdir(exist_ok=True, parents=True)
                
                chunk_dir = output_path / f"{Path(file_path).stem}_result"
                chunk_dir.mkdir(exist_ok=True)
                
                extracted = await ResultProcessor.download_and_extract(session, full_zip_url, str(chunk_dir))
                
                if not extracted:
                    logger.error("ä¸‹è½½è§£å‹å¤±è´¥")
                    print("âŒ ä¸‹è½½è§£å‹å¤±è´¥")
                    return None
                
                logger.info(f"ä¸‹è½½è§£å‹æˆåŠŸ: {extracted}")
                
                # 5. æ•´ç†è¾“å‡º
                logger.info("æ•´ç†è¾“å‡ºæ–‡ä»¶")
                file_name = Path(file_path).stem
                md_file = output_path / f"{file_name}.md"
                images_dir = output_path / f"{file_name}_images"
                
                # å¤åˆ¶Markdown
                source_md = ResultProcessor.find_markdown(extracted)
                if source_md:
                    shutil.copy(source_md, md_file)
                    logger.info(f"Markdownå·²å¤åˆ¶: {md_file}")
                    print(f"âœ… Markdown: {md_file}")
                
                # å¤åˆ¶å›¾ç‰‡
                source_images = Path(extracted) / "images"
                if source_images.exists():
                    if images_dir.exists():
                        shutil.rmtree(images_dir)
                    shutil.copytree(source_images, images_dir)
                    image_count = len(list(images_dir.glob("*")))
                    logger.info(f"å›¾ç‰‡å·²å¤åˆ¶: {image_count}ä¸ª")
                    print(f"âœ… å›¾ç‰‡: {images_dir} ({image_count}ä¸ª)")
                
                logger.info("å¤„ç†å®Œæˆ")
                return {
                    'source': file_path,
                    'source_type': 'url' if file_info['is_url'] else 'file',
                    'output': {
                        'markdown': str(md_file),
                        'images': str(images_dir) if images_dir.exists() else None
                    }
                }
        
        except Exception as e:
            logger.error(f"å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return None


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 mineru_async.py <file_path>")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    processor = MinerUAsyncProcessor(max_workers=10)
    result = asyncio.run(processor.process_file(file_path))
    
    if result:
        print(f"\nâœ… å¤„ç†æˆåŠŸ!")
        print(f"  Markdown: {result['output']['markdown']}")
        print(f"  å›¾ç‰‡: {result['output']['images']}")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥")
        sys.exit(1)
